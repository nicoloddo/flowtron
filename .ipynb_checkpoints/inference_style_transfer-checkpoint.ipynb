{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowtron Style Transfer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from flowtron import Flowtron\n",
    "from data import Data\n",
    "from train import update_params\n",
    "sys.path.insert(0, \"tacotron2\")\n",
    "sys.path.insert(0, \"tacotron2/waveglow\")\n",
    "from denoiser import Denoiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Flowtron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config.dummy_speaker_embedding=0\n",
      "dummy_speaker_embedding=0\n",
      "data_config.p_arpabet=1.0\n",
      "p_arpabet=1.0\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config.json\"\n",
    "params = [\"model_config.dummy_speaker_embedding=0\",\n",
    "          \"data_config.p_arpabet=1.0\"]\n",
    "\n",
    "with open(config_path) as f:\n",
    "    data = f.read()\n",
    "\n",
    "config = json.loads(data)\n",
    "update_params(config, params)\n",
    "\n",
    "data_config = config[\"data_config\"]\n",
    "model_config = config[\"model_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS IF USING libritts\n",
    "#model_config[\"n_speakers\"] = 123\n",
    "\n",
    "model_path = \"models/flowtron_ljs.pt\"\n",
    "state_dict = torch.load(model_path, map_location='cpu')['state_dict']\n",
    "model = Flowtron(**model_config)\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#waveglow_path = 'models/waveglow_256channels_universal_v5.pt'\u001b[39;00m\n\u001b[0;32m      2\u001b[0m waveglow_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/generator_v3.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m waveglow \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveglow_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m waveglow\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      5\u001b[0m denoiser \u001b[38;5;241m=\u001b[39m Denoiser(waveglow)\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "waveglow_path = 'models/waveglow_256channels_universal_v5.pt'\n",
    "waveglow = torch.load(waveglow_path)['model']\n",
    "_ = waveglow.eval().cuda()\n",
    "denoiser = Denoiser(waveglow).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download samples with surprised style and unzip them in the 'data' folder\n",
    "[Surprised samples](https://drive.google.com/file/d/100YJu80Y-k5katrwzzE6rFoEHJ2rLmkc/view?usp=sharing) https://drive.google.com/file/d/100YJu80Y-k5katrwzzE6rFoEHJ2rLmkc/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"happy\"\n",
    "dataset_path_surprised = 'data/surprised_samples/surprised_audiofilelist_text.txt'\n",
    "dataset_path = \"filelists/\" + str(emotion) +\"_reference_audios.txt\"\n",
    "disfluent_dataset_path = \"filelists/reducted_disfl_fakesinglespeaker_format_transcript.txt\"\n",
    "dataset = Data(\n",
    "    dataset_path_surprised,\n",
    "    **dict((k, v) for k, v in data_config.items() if k not in ['training_files', 'validation_files']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect z values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = []\n",
    "force_speaker_id = 0\n",
    "for i in range(len(dataset)):\n",
    "    mel, sid, text, attn_prior = dataset[i]\n",
    "    mel, sid, text = mel[None].cuda(), sid.cuda(), text[None].cuda()\n",
    "    if force_speaker_id > -1:\n",
    "        sid = sid * 0 + force_speaker_id\n",
    "    in_lens = torch.LongTensor([text.shape[1]]).cuda()\n",
    "    out_lens = torch.LongTensor([mel.shape[2]]).cuda()\n",
    "    with torch.no_grad():\n",
    "        z = model(mel, sid, text, in_lens, out_lens)[0]\n",
    "        z_values.append(z.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 0.0001\n",
    "sigma = 1.\n",
    "n_frames = 300\n",
    "aggregation_type = 'batch'\n",
    "\n",
    "if aggregation_type == 'time_and_batch':\n",
    "    z_mean = torch.cat([z.mean(dim=2) for z in z_values])\n",
    "    z_mean = torch.mean(z_mean, dim=0)[:, None]\n",
    "    ratio = len(z_values) / lambd\n",
    "    mu_posterior = (ratio * z_mean / (ratio + 1))\n",
    "elif aggregation_type == 'batch':    \n",
    "    for k in range(len(z_values)):\n",
    "        expand = z_values[k]\n",
    "        while expand.size(2) < n_frames:\n",
    "            expand = torch.cat((expand, z_values[k]), 2)\n",
    "        z_values[k] = expand[:, :, :n_frames]\n",
    "\n",
    "    z_mean = torch.mean(torch.cat(z_values, dim=0), dim=0)[None]\n",
    "    z_mean_size = z_mean.size()\n",
    "    z_mean = z_mean.flatten()\n",
    "    ratio = len(z_values) / float(lambd)\n",
    "    mu_posterior = (ratio * z_mean / (ratio + 1)).flatten()\n",
    "    mu_posterior = mu_posterior.view(80, -1)\n",
    "\n",
    "print(ratio)\n",
    "dist = Normal(mu_posterior.cpu(), sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_baseline = torch.FloatTensor(1, 80, n_frames).cuda().normal_() * sigma\n",
    "if aggregation_type == 'time_and_batch':\n",
    "    z_posterior = dist.sample([n_frames]).permute(2,1,0).cuda()\n",
    "elif aggregation_type == 'batch':\n",
    "    z_posterior = dist.sample().view(1, 80, -1)[..., :n_frames].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I live in London because um I like it\"\n",
    "text_encoded = dataset.get_text(text).cuda()[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform inference sampling the posterior and a standard gaussian baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 0\n",
    "speaker_id = torch.LongTensor([speaker]).cuda()\n",
    "with torch.no_grad():\n",
    "    mel_posterior = model.infer(z_posterior, speaker_id, text_encoded)[0]\n",
    "    mel_baseline = model.infer(z_baseline, speaker_id, text_encoded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 6))\n",
    "axes[0, 0].imshow(mel_posterior[0].cpu(), aspect='auto', origin='lower', interpolation='none')\n",
    "im = axes[0, 1].imshow(z_posterior[0].cpu(), aspect='auto', origin='lower', interpolation='none')\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "axes[1, 0].imshow(mel_baseline[0].cpu(), aspect='auto', origin='lower', interpolation='none')\n",
    "im = axes[1, 1].imshow(z_baseline[0].cpu(), aspect='auto', origin='lower', interpolation='none')\n",
    "plt.colorbar(im, ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    audio = denoiser(waveglow.infer(mel_posterior, sigma=0.75), 0.01)\n",
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=data_config['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    audio = denoiser(waveglow.infer(mel_baseline, sigma=0.75), 0.01)\n",
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=data_config['sampling_rate'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
