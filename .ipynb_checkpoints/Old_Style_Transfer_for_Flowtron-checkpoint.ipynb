{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygul_8CfZWXz"
   },
   "source": [
    "# Style Transfer Inference Demo for Flowtron on Google COLAB¶ \n",
    "\n",
    "Original code is by:\n",
    "\n",
    "Rafael Valle, Kevin Shih, Ryan Prenger and Bryan Catanzaro | NVIDIA\n",
    "\n",
    "The Google Colaboratory style trasfer code was written by;\n",
    "\n",
    "Bence Halpern | PhD Student | University of Amsterdam, TU Delft, Netherlands Cancer Institute\n",
    "\n",
    "**E-mail about info and discussions:** b.m.halpern[atttt]uva.nl\n",
    "\n",
    "## Intro\n",
    "This notebook requires a GPU runtime to run. Please select the menu option \"**Runtime**\" -> \"**Change runtime type**\", select \"**Hardware Accelerator**\" -> \"**GPU**\" and click \"**SAVE**\"\n",
    "\n",
    "## Model Description\n",
    "\n",
    "The TTS used in this colab is Flowtron. The original paper is:\n",
    "\n",
    "- VALLE, Rafael, et al. Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis. arXiv preprint arXiv:2005.05957, 2020.\n",
    "\n",
    "The style transfer method used is the robust reference audio-based method to perform emotional style transfer. To my knowledge, this was first done in the Tacotron 2 GST by Kwon et al. We use this method with Flowtron to get emotional audio. More detail about the reference audio-based method:\n",
    "\n",
    "- KWON, Ohsung, et al. An Effective Style Token Weight Control Technique for End-to-End Emotional Speech Synthesis. IEEE Signal Processing Letters, 2019, 26.9: 1383-1387.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The happy and sad reference emotional signals are from the RAVDESS dataset. \n",
    "\n",
    "Please cite their work if you use the emotional data in your work:\n",
    "- Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caV9rNth0t35"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "For your custom style transfer, you need to provide your own audio files and file lists. The easiest way you can learn how to do this is by mimicking the examples below. Upload the audio files and your file lists to your Google Drive and set it to public access.\n",
    "\n",
    "Check that the printed downloaded size agrees with the original size. If not, you might have made a mistake in the download link or you forgot to make it public.\n",
    "\n",
    "Don't forget to downsample your audios. You can use the bash script for that in the happy.zip. The Flowtron uses 22050 Hz and 16-bit depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kytmjZ1_kMK",
    "outputId": "75c8dc34-ba81-4253-fe4f-7a01bd69d55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting numpy==1.13.3\n",
      "  Downloading numpy-1.13.3.zip (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torch==1.5\n",
      "  Downloading torch-1.5.0-cp38-cp38-manylinux1_x86_64.whl (752.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.0/752.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15\u001b[0m\u001b[31m\n",
      "\u001b[0mCloning into 'flowtron'...\n",
      "remote: Enumerating objects: 108, done.\u001b[K\n",
      "remote: Total 108 (delta 0), reused 0 (delta 0), pack-reused 108\u001b[K\n",
      "Receiving objects: 100% (108/108), 2.54 MiB | 8.13 MiB/s, done.\n",
      "Resolving deltas: 100% (33/33), done.\n",
      "/content/flowtron\n",
      "Submodule 'apex' (https://github.com/NVIDIA/apex) registered for path 'apex'\n",
      "Submodule 'tacotron2' (https://github.com/NVIDIA/tacotron2) registered for path 'tacotron2'\n",
      "Cloning into '/content/flowtron/apex'...\n",
      "Cloning into '/content/flowtron/tacotron2'...\n",
      "Submodule path 'apex': checked out '9165b27fdf240f9bc08eac98b849a9d7c6308917'\n",
      "Submodule path 'tacotron2': checked out '6f435f7f29c3e1553cf2dd7ca2daf56903b20c39'\n",
      "/content/flowtron/tacotron2\n",
      "Submodule 'waveglow' (https://github.com/NVIDIA/waveglow) registered for path 'waveglow'\n",
      "Cloning into '/content/flowtron/tacotron2/waveglow'...\n",
      "Submodule path 'waveglow': checked out '2fd4e63e2918012f55eac2c8a8e75622a39741be'\n",
      "/content/flowtron\n",
      "apex\t\t\t    flowtron.py\n",
      "audio_processing.py\t    inference.py\n",
      "config.json\t\t    LICENSE\n",
      "data\t\t\t    README.md\n",
      "data.py\t\t\t    requirements.txt\n",
      "distributed.py\t\t    Style_Transfer_for_Flowtron.ipynb\n",
      "filelists\t\t    tacotron2\n",
      "flowtron_logger.py\t    text\n",
      "flowtron_plotting_utils.py  train.py\n",
      "It took  2.6sec to download 2.1 MB happy.zip \n",
      "It took  1.7sec to download 1.8 MB sad.zip \n",
      "It took  1.21sec to download 1.6 KB happy_reference_audios.txt \n",
      "It took  0.8sec to download 1.5 KB sad_reference_audios.txt \n",
      "Archive:  happy.zip\n",
      "   creating: data/happy/\n",
      "  inflating: data/happy/03-01-03-02-02-01-04.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-08.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-01.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-06.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-18.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-07.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-09.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-20.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-21.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-13.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-03.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-05.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-16.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-24.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-19.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-15.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-17.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-12.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-10.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-11.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-22.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-23.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-14.wav  \n",
      "  inflating: data/happy/03-01-03-02-02-01-02.wav  \n",
      "  inflating: data/happy/conversion.sh  \n",
      "Archive:  sad.zip\n",
      "   creating: data/sad/\n",
      "  inflating: data/sad/03-01-04-02-02-01-05.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-19.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-17.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-03.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-24.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-02.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-22.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-20.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-18.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-12.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-16.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-13.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-06.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-23.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-15.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-10.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-11.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-09.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-14.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-08.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-21.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-07.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-04.wav  \n",
      "  inflating: data/sad/03-01-04-02-02-01-01.wav  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# This is to make sure that the venv corresponding to the correct\n",
    "# ipython kernel is used\n",
    "#!{sys.executable} -m pip install numpy==1.13.3 torch==1.5 tensorflow==1.15 inflect==0.2.5 librosa==0.6.0 scipy==1.0.0 tensorboardX==1.1 Unidecode==1.0.22 pillow \n",
    "\n",
    "# We need to check out a certain commit here to avoid changes\n",
    "'''\n",
    "!git clone https://github.com/karkirowle/flowtron.git\n",
    "%cd flowtron\n",
    "!git submodule init\n",
    "!git submodule update\n",
    "%cd tacotron2\n",
    "!git submodule update --init\n",
    "%cd ..\n",
    "'''\n",
    "\n",
    "\n",
    "!ls\n",
    "# This is ported from https://github.com/yhgon/mellotron/blob/master/inference_colab.ipynb\n",
    "# This downloads the style transfer data and the trained Flowtron with vocoder\n",
    "!wget -N  -q https://raw.githubusercontent.com/yhgon/colab_utils/master/gfile.py\n",
    "#!python gfile.py -u 'https://drive.google.com/open?id=1KhJcPawFgmfvwV7tQAOeC253rYstLrs8' -f 'flowtron_libritts.pt'\n",
    "#!python gfile.py -u 'https://drive.google.com/open?id=1Cjd6dK_eFz6DE0PKXKgKxrzTUqzzUDW-' -f 'flowtron_ljs.pt'\n",
    "\n",
    "#!python gfile.py -u 'https://drive.google.com/file/d/1_g5X3KSCJIsXM-sRRjZHlObe48HhFtHB/view?usp=sharing' -f 'waveglow_256channels_v4.pt'\n",
    "\n",
    "\n",
    "\n",
    "!python gfile.py -u 'https://drive.google.com/open?id=1c1gPs4sGbFMoqvM_OgvDD4k4cGkfPWUQ' -f 'happy.zip'\n",
    "!python gfile.py -u 'https://drive.google.com/open?id=1LMIXuqz12PnJNBN5L2-95-jpLwoQX4_F' -f 'sad.zip'\n",
    "\n",
    "!python gfile.py -u 'https://drive.google.com/open?id=1ncaU1lYqcDIhRwJmWtxODZDHykSYsJKt' -f 'happy_reference_audios.txt'\n",
    "!python gfile.py -u 'https://drive.google.com/open?id=1JiSkb2jW8dfGELknmPhJ1-BOxlAJ6niQ' -f 'sad_reference_audios.txt'\n",
    "\n",
    "!unzip happy.zip -d data\n",
    "!unzip sad.zip -d data\n",
    "\n",
    "!mv happy_reference_audios.txt filelists/happy_reference_audios.txt\n",
    "!mv sad_reference_audios.txt filelists/sad_reference_audios.txt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "verT7i9fAhWt",
    "outputId": "e8b96490-4974-409d-e0fa-1663cfd11d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "['requirements.txt', 'train.py', 'tacotron2', 'happy.zip', 'filelists', 'data', 'apex', 'gfile.py', 'distributed.py', 'sad.zip', 'flowtron_logger.py', 'Style_Transfer_for_Flowtron.ipynb', 'inference.py', 'LICENSE', 'text', '.git', '.gitmodules', 'audio_processing.py', 'flowtron_plotting_utils.py', 'config.json', 'flowtron.py', 'README.md', 'data.py']\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.6\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/flowtron/text/__init__.py:77: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
      "/content/flowtron/text/__init__.py:77: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
     ]
    }
   ],
   "source": [
    "#from unidecode import unidecode\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "print(os.listdir())\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Matplotlib might need to be downgraded?\n",
    "!pip install unidecode\n",
    "!pip install tensorboardX\n",
    "import unidecode\n",
    "\n",
    "from flowtron import Flowtron\n",
    "from torch.utils.data import DataLoader\n",
    "from data import Data, load_wav_to_torch\n",
    "from train import update_params\n",
    "\n",
    "sys.path.insert(0, \"tacotron2\")\n",
    "sys.path.insert(0, \"tacotron2/waveglow\")\n",
    "from glow import WaveGlow\n",
    "from scipy.io.wavfile import write\n",
    "from torch.nn import ReplicationPad1d, ReflectionPad1d\n",
    "from glob import glob\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import IPython\n",
    "from data import DataCollate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LREx8RyTBS0b"
   },
   "outputs": [],
   "source": [
    "def infer(flowtron_path, waveglow_path, text, speaker_id, n_frames, sigma,\n",
    "          seed,emotion,utterance=None):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # load waveglow\n",
    "    waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
    "    waveglow.cuda().half()\n",
    "    for k in waveglow.convinv:\n",
    "        k.float()\n",
    "    waveglow.eval()\n",
    "\n",
    "    # load flowtron\n",
    "    model = Flowtron(**model_config).cuda()\n",
    "    state_dict = torch.load(flowtron_path, map_location='cpu')['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(\"Loaded checkpoint '{}')\" .format(flowtron_path))\n",
    "\n",
    "    ignore_keys = ['training_files', 'validation_files']\n",
    "    trainset = Data(\n",
    "        data_config['training_files'],\n",
    "        **dict((k, v) for k, v in data_config.items() if k not in ignore_keys))\n",
    "    speaker_vecs = trainset.get_speaker_id(speaker_id).cuda()\n",
    "\n",
    "    styleset = Data(\"filelists/\" + str(emotion) +\"_reference_audios.txt\",\n",
    "                    **dict((k, v) for k, v in data_config.items() if k not in ignore_keys))\n",
    "\n",
    "    print(len(styleset))\n",
    " # Feeding the dataset in one batch: modify if you have larger datast\n",
    "    batch_size = len(styleset)\n",
    "\n",
    "    # Sampler is assumed none for COLAB, because n_gpu=1\n",
    "    style_loader = DataLoader(styleset, num_workers=1, shuffle=False,\n",
    "                              sampler=None, batch_size=batch_size,\n",
    "                              pin_memory=False, collate_fn=DataCollate())\n",
    "\n",
    "    speaker_vecs = trainset.get_speaker_id(speaker_id).cuda()\n",
    "\n",
    "    text = trainset.get_text(text).cuda()\n",
    "    synth_speaker_vecs = speaker_vecs[None]\n",
    "    text = text[None]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        for batch in style_loader:\n",
    "            mel, speaker_vecs, utt_text, in_lens, out_lens, gate_target = batch\n",
    "\n",
    "            mel, speaker_vecs, utt_text = mel.cuda(), speaker_vecs.cuda(), utt_text.cuda()\n",
    "            in_lens, out_lens, gate_target = in_lens.cuda(), out_lens.cuda(), gate_target.cuda()\n",
    "\n",
    "            residual, _, _, _, _, _, _ = model.forward(mel, speaker_vecs, utt_text, in_lens, out_lens)\n",
    "            residual = residual.permute(1, 2, 0)\n",
    "\n",
    "        # At this stage the latent vectors are zero-padded which is not appropriate, because it violates the assumption\n",
    "        # of Gaussian latent space, leading to artefacts.\n",
    "\n",
    "\n",
    "\n",
    "        residual_accumulator = torch.zeros((residual.shape[0], 80, n_frames)).to(\"cuda\")\n",
    "\n",
    "        for i in range(residual.shape[0]):\n",
    "            current_len = out_lens[i].cpu().numpy()\n",
    "\n",
    "            if current_len < n_frames:\n",
    "                num_tile = int(np.ceil(n_frames/current_len))\n",
    "                residual_accumulator[i,:,:] = torch.repeat_interleave(residual[i,:,:current_len],repeats=num_tile,dim=1)[:,:n_frames]\n",
    "\n",
    "        residual_accumulator = torch.mean(residual_accumulator,dim=0)[None,:,:]\n",
    "       \n",
    "        average_over_time = False\n",
    "        if not average_over_time:\n",
    "            dist = Normal(residual_accumulator, sigma)\n",
    "            z_style = dist.sample()\n",
    "        else:\n",
    "            print(residual_accumulator.shape)\n",
    "            residual_accumulator = residual_accumulator.mean(dim=2)\n",
    "            dist = Normal(residual_accumulator,sigma)\n",
    "            z_style = dist.sample((n_frames,)).permute(1,2,0)\n",
    "\n",
    "        mels, attentions = model.infer(z_style, synth_speaker_vecs, text)\n",
    "\n",
    "    for k in range(len(attentions)):\n",
    "        attention = torch.cat(attentions[k]).cpu().numpy()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "        axes[0].imshow(mels[0].cpu().numpy(), origin='bottom', aspect='auto')\n",
    "        axes[1].imshow(attention[:, 0].transpose(), origin='bottom', aspect='auto')\n",
    "        fig.savefig('sid{}_sigma{}_attnlayer{}.png'.format(speaker_id, sigma, k))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    audio = waveglow.infer(mels.half(), sigma=0.8).float()\n",
    "    audio = audio.cpu().numpy()[0]\n",
    "    # normalize audio for now\n",
    "    audio = audio / np.abs(audio).max()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "lOYvZooE_b68",
    "outputId": "4dd37d48-a5a8-4d29-dea4-e492dc2081f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint '/content/drive/MyDrive/Colab Notebooks/Flowtron/flowtron_libritts.pt')\n",
      "Number of speakers : 123\n",
      "Number of speakers : 1\n",
      "24\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6383474fbc1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# I don't know what is going\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_notebook_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'flowtron_libritts.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_notebook_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'waveglow_256channels_v4.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{H} {E} {L} {L}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeaker_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6918b4e41a8f>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(flowtron_path, waveglow_path, text, speaker_id, n_frames, sigma, seed, emotion, utterance)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0min_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/flowtron/flowtron.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mel, speaker_vecs, text, in_lens, out_lens)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mspeaker_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeaker_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/flowtron/flowtron.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, in_lens)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor"
     ]
    }
   ],
   "source": [
    "base_notebook_path = '/content/drive/MyDrive/Colab Notebooks/Flowtron/'\n",
    "\n",
    "# Parse configs. Globals are never nice, but we use it anyway\n",
    "with open(\"config.json\") as f:\n",
    "  data = f.read()\n",
    "\n",
    "global config\n",
    "config = json.loads(data)\n",
    "#update_params(config, args.params)\n",
    "\n",
    "data_config = config[\"data_config\"]\n",
    "global model_config\n",
    "model_config = config[\"model_config\"]\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Emotion parameter, happy and sad is implemented\n",
    "emotion=\"happy\"\n",
    "#emotion=\"sad\"\n",
    "\n",
    "# DEFAULT SETUP: LJS\n",
    "speaker_id=0\n",
    "\n",
    "# LIBRITTS SETUP - UNCOMMENT\n",
    "data_config[\"training_files\"] = \"filelists/libritts_train_clean_100_audiopath_text_sid_shorterthan10s_atleast5min_train_filelist.txt\"\n",
    "model_config[\"n_speakers\"] = 123\n",
    "speaker_id=40 \n",
    "\n",
    "\n",
    "# There are some utterances that don't work, this one is tested, feel free to\n",
    "# experiment, but don't ask why it doesn't work! \n",
    "\n",
    "# I don't know what is going\n",
    "audio = infer(base_notebook_path + 'flowtron_libritts.pt', base_notebook_path + 'waveglow_256channels_v4.pt',\"{H} {E} {L} {L}\", speaker_id=speaker_id,n_frames=400, sigma=0.01, seed=1,emotion=emotion)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(audio[:])\n",
    "\n",
    "IPython.display.Audio(audio[:],rate=22050)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
