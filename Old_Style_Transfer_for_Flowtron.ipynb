{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygul_8CfZWXz"
   },
   "source": [
    "# Style Transfer Inference Demo for Flowtron on Google COLABÂ¶ \n",
    "\n",
    "Original code is by:\n",
    "\n",
    "Rafael Valle, Kevin Shih, Ryan Prenger and Bryan Catanzaro | NVIDIA\n",
    "\n",
    "The Google Colaboratory style trasfer code was written by;\n",
    "\n",
    "Bence Halpern | PhD Student | University of Amsterdam, TU Delft, Netherlands Cancer Institute\n",
    "\n",
    "**E-mail about info and discussions:** b.m.halpern[atttt]uva.nl\n",
    "\n",
    "## Intro\n",
    "This notebook requires a GPU runtime to run. Please select the menu option \"**Runtime**\" -> \"**Change runtime type**\", select \"**Hardware Accelerator**\" -> \"**GPU**\" and click \"**SAVE**\"\n",
    "\n",
    "## Model Description\n",
    "\n",
    "The TTS used in this colab is Flowtron. The original paper is:\n",
    "\n",
    "- VALLE, Rafael, et al. Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis. arXiv preprint arXiv:2005.05957, 2020.\n",
    "\n",
    "The style transfer method used is the robust reference audio-based method to perform emotional style transfer. To my knowledge, this was first done in the Tacotron 2 GST by Kwon et al. We use this method with Flowtron to get emotional audio. More detail about the reference audio-based method:\n",
    "\n",
    "- KWON, Ohsung, et al. An Effective Style Token Weight Control Technique for End-to-End Emotional Speech Synthesis. IEEE Signal Processing Letters, 2019, 26.9: 1383-1387.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The happy and sad reference emotional signals are from the RAVDESS dataset. \n",
    "\n",
    "Please cite their work if you use the emotional data in your work:\n",
    "- Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caV9rNth0t35"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "For your custom style transfer, you need to provide your own audio files and file lists. The easiest way you can learn how to do this is by mimicking the examples below. Upload the audio files and your file lists to your Google Drive and set it to public access.\n",
    "\n",
    "Check that the printed downloaded size agrees with the original size. If not, you might have made a mistake in the download link or you forgot to make it public.\n",
    "\n",
    "Don't forget to downsample your audios. You can use the bash script for that in the happy.zip. The Flowtron uses 22050 Hz and 16-bit depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "verT7i9fAhWt",
    "outputId": "e8b96490-4974-409d-e0fa-1663cfd11d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\anaconda3\\envs\\psyche\\python.exe\n",
      "['.git', '.gitmodules', '.ipynb_checkpoints', 'apex', 'audio_processing.py', 'config.json', 'data', 'data.py', 'distributed.py', 'Dockerfile', 'filelists', 'flowtron.py', 'flowtron_logger.py', 'flowtron_plotting_utils.py', 'inference.py', 'inference_style_transfer.ipynb', 'jupyter_in_psyche.bat', 'LICENSE', 'models', 'radam.py', 'README.md', 'README_nico.txt', 'requirements.txt', 'results', 'Style_Transfer_for_Flowtron.ipynb', 'tacotron2', 'text', 'train.py', '__pycache__']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#from unidecode import unidecode\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "print(os.listdir())\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Matplotlib might need to be downgraded?\n",
    "import unidecode\n",
    "\n",
    "from flowtron import Flowtron\n",
    "from torch.utils.data import DataLoader\n",
    "from data import Data, load_wav_to_torch\n",
    "from train import update_params\n",
    "\n",
    "sys.path.insert(0, \"tacotron2\")\n",
    "sys.path.insert(0, \"tacotron2/waveglow\")\n",
    "from glow import WaveGlow\n",
    "from scipy.io.wavfile import write\n",
    "from torch.nn import ReplicationPad1d, ReflectionPad1d\n",
    "from glob import glob\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import IPython\n",
    "from data import DataCollate\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LREx8RyTBS0b"
   },
   "outputs": [],
   "source": [
    "def infer(flowtron_path, waveglow_path, text, speaker_id, n_frames, sigma,\n",
    "          seed,emotion,utterance=None):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # load waveglow\n",
    "    waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
    "    waveglow.cuda().half()\n",
    "    for k in waveglow.convinv:\n",
    "        k.float()\n",
    "    waveglow.eval()\n",
    "\n",
    "    # load flowtron\n",
    "    model = Flowtron(**model_config).cuda()\n",
    "    state_dict = torch.load(flowtron_path, map_location='cpu')['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(\"Loaded checkpoint '{}')\" .format(flowtron_path))\n",
    "\n",
    "    ignore_keys = ['training_files', 'validation_files']\n",
    "    trainset = Data(\n",
    "        data_config['training_files'],\n",
    "        **dict((k, v) for k, v in data_config.items() if k not in ignore_keys))\n",
    "    speaker_vecs = trainset.get_speaker_id(speaker_id).cuda()\n",
    "\n",
    "    styleset = Data(\"filelists/\" + str(emotion) +\"_reference_audios.txt\",\n",
    "                    **dict((k, v) for k, v in data_config.items() if k not in ignore_keys))\n",
    "\n",
    "    print(len(styleset))\n",
    " # Feeding the dataset in one batch: modify if you have larger datast\n",
    "    batch_size = len(styleset)\n",
    "\n",
    "    # Sampler is assumed none for COLAB, because n_gpu=1\n",
    "    style_loader = DataLoader(styleset, num_workers=1, shuffle=False,\n",
    "                              sampler=None, batch_size=batch_size,\n",
    "                              pin_memory=False, collate_fn=DataCollate())\n",
    "\n",
    "    speaker_vecs = trainset.get_speaker_id(speaker_id).cuda()\n",
    "\n",
    "    text = trainset.get_text(text).cuda()\n",
    "    synth_speaker_vecs = speaker_vecs[None]\n",
    "    text = text[None]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        for batch in style_loader:\n",
    "            mel, speaker_vecs, utt_text, in_lens, out_lens, gate_target = batch\n",
    "\n",
    "            mel, speaker_vecs, utt_text = mel.cuda(), speaker_vecs.cuda(), utt_text.cuda()\n",
    "            in_lens, out_lens, gate_target = in_lens.cuda(), out_lens.cuda(), gate_target.cuda()\n",
    "\n",
    "            residual, _, _, _, _, _, _ = model.forward(mel, speaker_vecs, utt_text, in_lens, out_lens)\n",
    "            residual = residual.permute(1, 2, 0)\n",
    "\n",
    "        # At this stage the latent vectors are zero-padded which is not appropriate, because it violates the assumption\n",
    "        # of Gaussian latent space, leading to artefacts.\n",
    "\n",
    "\n",
    "\n",
    "        residual_accumulator = torch.zeros((residual.shape[0], 80, n_frames)).to(\"cuda\")\n",
    "\n",
    "        for i in range(residual.shape[0]):\n",
    "            current_len = out_lens[i].cpu().numpy()\n",
    "\n",
    "            if current_len < n_frames:\n",
    "                num_tile = int(np.ceil(n_frames/current_len))\n",
    "                residual_accumulator[i,:,:] = torch.repeat_interleave(residual[i,:,:current_len],repeats=num_tile,dim=1)[:,:n_frames]\n",
    "\n",
    "        residual_accumulator = torch.mean(residual_accumulator,dim=0)[None,:,:]\n",
    "       \n",
    "        average_over_time = False\n",
    "        if not average_over_time:\n",
    "            dist = Normal(residual_accumulator, sigma)\n",
    "            z_style = dist.sample()\n",
    "        else:\n",
    "            print(residual_accumulator.shape)\n",
    "            residual_accumulator = residual_accumulator.mean(dim=2)\n",
    "            dist = Normal(residual_accumulator,sigma)\n",
    "            z_style = dist.sample((n_frames,)).permute(1,2,0)\n",
    "\n",
    "        mels, attentions = model.infer(z_style, synth_speaker_vecs, text)\n",
    "\n",
    "    for k in range(len(attentions)):\n",
    "        attention = torch.cat(attentions[k]).cpu().numpy()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "        axes[0].imshow(mels[0].cpu().numpy(), origin='bottom', aspect='auto')\n",
    "        axes[1].imshow(attention[:, 0].transpose(), origin='bottom', aspect='auto')\n",
    "        fig.savefig('sid{}_sigma{}_attnlayer{}.png'.format(speaker_id, sigma, k))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    audio = waveglow.infer(mels.half(), sigma=0.8).float()\n",
    "    audio = audio.cpu().numpy()[0]\n",
    "    # normalize audio for now\n",
    "    audio = audio / np.abs(audio).max()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "lOYvZooE_b68",
    "outputId": "4dd37d48-a5a8-4d29-dea4-e492dc2081f0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse configs. Globals are never nice, but we use it anyway\n",
    "with open(\"config.json\") as f:\n",
    "  data = f.read()\n",
    "\n",
    "global config\n",
    "config = json.loads(data)\n",
    "#update_params(config, args.params)\n",
    "\n",
    "data_config = config[\"data_config\"]\n",
    "global model_config\n",
    "model_config = config[\"model_config\"]\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Emotion parameter, happy and sad is implemented\n",
    "emotion=\"happy\"\n",
    "#emotion=\"sad\"\n",
    "\n",
    "# DEFAULT SETUP: LJS\n",
    "speaker_id=0\n",
    "\n",
    "# LIBRITTS SETUP - UNCOMMENT\n",
    "#data_config[\"training_files\"] = \"filelists/libritts_train_clean_100_audiopath_text_sid_shorterthan10s_atleast5min_train_filelist.txt\"\n",
    "#model_config[\"n_speakers\"] = 123\n",
    "#speaker_id=40 \n",
    "\n",
    "\n",
    "# There are some utterances that don't work, this one is tested, feel free to\n",
    "# experiment, but don't ask why it doesn't work! \n",
    "\n",
    "# I don't know what is going\n",
    "audio = infer('models/flowtron_ljs.pt', 'models/waveglow_256channels_v4.pt',\"{H} {E} {L} {L}\", speaker_id=speaker_id,n_frames=400, sigma=0.01, seed=1,emotion=emotion)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(audio[:])\n",
    "\n",
    "IPython.display.Audio(audio[:],rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio[:],rate=22050)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
